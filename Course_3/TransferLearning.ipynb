{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TransferLearning.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMeXW79Mq8hSR7OWFw9+xiF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"rMrjAVVLzyGc","colab_type":"code","colab":{}},"source":["from torchvision import models, transforms\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","vgg = models.vgg16(pretrained=True)\n","device = torch.device(\"cuda\")\n","vgg = vgg.to(device)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fF_hkX_7z8eX","colab_type":"code","colab":{}},"source":["# deshabilito gradiente de todos los parametros de la red\n","# esto es porque el entrenamiento solo se realizaráá para\n","# la capa fully conected que se agrega al final\n","for param in vgg.parameters():\n","  param.requires_grad = False"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UvsF_Jby0OsG","colab_type":"code","colab":{}},"source":["# 3. extraccióón de la úúltima capa de la red\n","# VGG fue construida usando nn.Sequential, por lo que la \"úúltima capa\"\n","# en realidad son muchas capas juntas\n","# por ello se llaman 2 veces el méétodo children\n","last_sequential_layer = list(vgg.children())[-1]\n","*list_of_layers, last_layer = list(last_sequential_layer.children())\n","in_features = last_layer.in_features"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"V6VL2OJP1wwZ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":121},"outputId":"b18734e6-16eb-46b0-d942-2f28aec246f3","executionInfo":{"status":"ok","timestamp":1590362726804,"user_tz":300,"elapsed":538,"user":{"displayName":"Nicolás Roldán","photoUrl":"","userId":"15793920032108430403"}}},"source":["list_of_layers"],"execution_count":96,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Linear(in_features=25088, out_features=4096, bias=True),\n"," ReLU(inplace=True),\n"," Dropout(p=0.5, inplace=False),\n"," Linear(in_features=4096, out_features=4096, bias=True),\n"," ReLU(inplace=True),\n"," Dropout(p=0.5, inplace=False)]"]},"metadata":{"tags":[]},"execution_count":96}]},{"cell_type":"code","metadata":{"id":"ESsejVkO1-Py","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":139},"outputId":"fc6c39b4-f1ae-4e2f-a37f-d0c5f811d205","executionInfo":{"status":"ok","timestamp":1590362727589,"user_tz":300,"elapsed":904,"user":{"displayName":"Nicolás Roldán","photoUrl":"","userId":"15793920032108430403"}}},"source":["list_of_layers +  [nn.Linear(in_features,6)]"],"execution_count":97,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Linear(in_features=25088, out_features=4096, bias=True),\n"," ReLU(inplace=True),\n"," Dropout(p=0.5, inplace=False),\n"," Linear(in_features=4096, out_features=4096, bias=True),\n"," ReLU(inplace=True),\n"," Dropout(p=0.5, inplace=False),\n"," Linear(in_features=4096, out_features=6, bias=True)]"]},"metadata":{"tags":[]},"execution_count":97}]},{"cell_type":"code","metadata":{"id":"w7cOzxTz1CF9","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"7da2f12e-a5bd-432b-89bf-74dd8e6a25c8","executionInfo":{"status":"ok","timestamp":1590362727590,"user_tz":300,"elapsed":508,"user":{"displayName":"Nicolás Roldán","photoUrl":"","userId":"15793920032108430403"}}},"source":["in_features"],"execution_count":98,"outputs":[{"output_type":"execute_result","data":{"text/plain":["4096"]},"metadata":{"tags":[]},"execution_count":98}]},{"cell_type":"code","metadata":{"id":"4J_Hdjtz1c0l","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":173},"outputId":"6107fdc5-de03-4929-d14f-1631ca70b4d3","executionInfo":{"status":"ok","timestamp":1590362728459,"user_tz":300,"elapsed":985,"user":{"displayName":"Nicolás Roldán","photoUrl":"","userId":"15793920032108430403"}}},"source":["vgg.classifier"],"execution_count":99,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Sequential(\n","  (0): Linear(in_features=25088, out_features=4096, bias=True)\n","  (1): ReLU(inplace=True)\n","  (2): Dropout(p=0.5, inplace=False)\n","  (3): Linear(in_features=4096, out_features=4096, bias=True)\n","  (4): ReLU(inplace=True)\n","  (5): Dropout(p=0.5, inplace=False)\n","  (6): Linear(in_features=4096, out_features=1000, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":99}]},{"cell_type":"code","metadata":{"id":"ILie7P8V1Dqw","colab_type":"code","colab":{}},"source":["# se mantienen las úúltimas capas que iban en nn.Sequential\n","# menos la úúltima donde esta se reemplaza por la que se quiere entrenar\n","vgg.fc = nn.Linear(in_features,6)\n","vgg.fc.requires_grad = True\n","vgg.classifier = nn.Sequential(*(list_of_layers + [vgg.fc]))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AserAoS-1V8N","colab_type":"code","colab":{}},"source":["# entrenamiento\n","# en general las redes pre-entrenadas fueron entrenadas con datasets que teníían cierta\n","# media y desviacióón estandar, es recomendable usar estas en el pipeline\n","# de preprocesamiento"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"e3-Gbr_n3iQ1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"outputId":"c1828513-4e0f-4202-c001-389bca864496","executionInfo":{"status":"ok","timestamp":1590362729638,"user_tz":300,"elapsed":940,"user":{"displayName":"Nicolás Roldán","photoUrl":"","userId":"15793920032108430403"}}},"source":["from google.colab import drive\n","drive.mount(\"/gdrive\")"],"execution_count":102,"outputs":[{"output_type":"stream","text":["Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zzt9bJqk3pIS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"6d7e3861-cf69-4430-e5cd-1856c4a89088","executionInfo":{"status":"ok","timestamp":1590362733028,"user_tz":300,"elapsed":3549,"user":{"displayName":"Nicolás Roldán","photoUrl":"","userId":"15793920032108430403"}}},"source":["# visualization (splitted dataset)\n","!ls \"/gdrive/My Drive/dl-pytorch/datasets/64x64_SIGNS\""],"execution_count":103,"outputs":[{"output_type":"stream","text":["test_signs  train_signs  val_signs\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GLZpl26S3sqv","colab_type":"code","colab":{}},"source":["import sys\n","sys.path.append(\"/gdrive/My Drive/dl-pytorch/\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6RvBJvZH3tlL","colab_type":"code","colab":{}},"source":["# implementar un dataset hereda de otra clase que viene en pytorch\n","# hay que implementar 3 métodos\n","# 1. init: set de atributos del dataset (rutas hacia las imáágenes)\n","# 2. length: cuantas imáágenes tiene el dataset?\n","# 3. get_item: acceder a las listas\n","# # pipeline de procesamiento (transform)\n","import os\n","from PIL import Image\n","from torch.utils.data import Dataset, DataLoader\n","\n","class SIGNSDataset(Dataset):\n","  def __init__(self,base_dir, split=\"train\", transform=None):\n","    path = os.path.join(base_dir, \"{}_signs\".format(split))\n","    files = os.listdir(path)\n","\n","    self.filenames = [os.path.join(path,f) for f in files if f.endswith(\".jpg\")]\n","    # labels\n","    self.targets = [int(f[0]) for f in files]\n","    self.transform = transform\n","\n","  # retorna la cantidad de imáágenes\n","  def __len__(self):\n","    return len(self.filenames)\n","\n","  # retorna la imagen y el target\n","  def __getitem__(self,idx):  \n","    # carga la imagen\n","    image = Image.open(self.filenames[idx])\n","    # si se da algún pipeline, se aplica\n","    if self.transform:\n","      image = self.transform(image)\n","    return image,self.targets[idx]\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rcol49WX4ITE","colab_type":"code","colab":{}},"source":["# pipeline de pre-procesamiento\n","transform = transforms.Compose(\n","    [\n","     transforms.RandomHorizontalFlip(),\n","     transforms.ToTensor(),\n","     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","    ]\n",")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6S-U3SHy3vlL","colab_type":"code","colab":{}},"source":["trainset = SIGNSDataset(\"/gdrive/My Drive/dl-pytorch/datasets/64x64_SIGNS\",split=\"train\",transform= transform)\n","trainloader = DataLoader(trainset,batch_size=32)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"H16vbOgd38t5","colab_type":"code","colab":{}},"source":["valset = SIGNSDataset(\"/gdrive/My Drive/dl-pytorch/datasets/64x64_SIGNS\",split=\"val\",transform= transform)\n","valloader = DataLoader(valset,batch_size=32)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iugzivXv3-rf","colab_type":"code","colab":{}},"source":["testset = SIGNSDataset(\"/gdrive/My Drive/dl-pytorch/datasets/64x64_SIGNS\",split=\"test\",transform= transform)\n","testloader = DataLoader(testset,batch_size=32)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KhaIKVMv4ARE","colab_type":"code","colab":{}},"source":["dataloaders = {'train':trainloader,\n","              'val':valloader,\n","              'test':testloader}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"P_G_AI-aAJ6O","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"a7bd10f0-39df-4239-ffed-2f94e6667cdc","executionInfo":{"status":"ok","timestamp":1590362736543,"user_tz":300,"elapsed":776,"user":{"displayName":"Nicolás Roldán","photoUrl":"","userId":"15793920032108430403"}}},"source":["vgg.parameters()"],"execution_count":111,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<generator object Module.parameters at 0x7f4e70666308>"]},"metadata":{"tags":[]},"execution_count":111}]},{"cell_type":"code","metadata":{"id":"tg1Ab65uAFn0","colab_type":"code","colab":{}},"source":["loss_fn = nn.NLLLoss()\n","optimizer = optim.SGD(vgg.parameters(), lr=1e-3, momentum = 0.9)\n","vgg = vgg.to(device)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z0RLd4mw5Zu1","colab_type":"code","colab":{}},"source":["# tip: clase para logeo de las méétricas en cada iteración\n","class RunningMetric():\n","  def __init__(self):\n","    self.Suma = 0\n","    self.N = 0\n","\n","  def update(self, val, size):\n","    self.Suma += val\n","    self.N += size\n","\n","  def __call__(self):\n","    return self.Suma/float(self.N)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Imri3KHF5msl","colab_type":"code","colab":{}},"source":["def train_and_evaluate(model, optimizer, loss_fn, dataloaders, device, num_epoch = 10, lr = 0.001):\n","  for g in optimizer.param_groups:\n","    g[\"lr\"] = lr # permite modificar le learning rate\n","\n","  \n","\n","  for epoch in range(num_epoch):\n","    print(\"Epoch {}/{}\".format(epoch+1, num_epoch))\n","    print(\"-\"*10)\n","\n","    for phase in [\"train\",\"val\"]:\n","      # dataloader = dataloaders[phase]\n","      if phase == \"train\":\n","        model.train()\n","      else:\n","        model.eval()\n","\n","      # dataloader = dataloaders[phase]\n","\n","      running_loss = RunningMetric()\n","      running_acc = RunningMetric()\n","\n","      # running_loss = 0.0\n","      # running_corrects = 0\n","\n","\n","      for inputs, targets in dataloaders[phase]:\n","        #primero: input y target a gpu\n","        inputs, targets = inputs.to(device), targets.to(device)\n","        # para el nuevo batch se lleva a cero los gradientes\n","        optimizer.zero_grad()\n","        # en las fases de test no necesitamos actualizar gradientes\n","        with torch.set_grad_enabled(phase == \"train\"):\n","          # outputs = net(inputs)\n","          \n","          # _, preds = torch.max(outputs,1) # target predicho\n","          outputs = vgg(inputs)\n","          _, preds = torch.max(outputs,1)\n","\n","          loss = loss_fn(outputs, targets) # calcula la pérdida\n","          if phase == \"train\":\n","            loss.backward() # gradients with backpropagation\n","            optimizer.step() # actualiza los parametros \n","\n","        batch_size = inputs.size()[0]\n","        running_loss.update(loss.item()*batch_size, batch_size)\n","        running_acc.update(torch.sum(preds == targets).float(), batch_size)\n","        # running_loss += loss.item() * inputs.size(0)\n","        # running_corrects += torch.sum(preds == targets.data)\n","      \n","      \n","      # epoch_loss = running_loss / len(dataloaders[phase])\n","      # epoch_acc = running_corrects.double() / len(dataloaders[phase])\n","      # print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n","      #     phase, epoch_loss, epoch_acc))\n","      print(\"Phase: {} Loss: {:.4f} Acc: {:.4f}\".format(phase,running_loss(),running_acc()))\n","  \n","  return model\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"99OIQQKP5Xmh","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"b0832994-51b9-4a05-bec5-97c56127ca42","executionInfo":{"status":"ok","timestamp":1590362928636,"user_tz":300,"elapsed":188879,"user":{"displayName":"Nicolás Roldán","photoUrl":"","userId":"15793920032108430403"}}},"source":["train_and_evaluate(vgg, optimizer, loss_fn, dataloaders, device, num_epoch = 100)"],"execution_count":115,"outputs":[{"output_type":"stream","text":["Epoch 1/100\n","----------\n","Phase: train Loss: -31.7996 Acc: 0.4850\n","Phase: val Loss: -81.2977 Acc: 0.6435\n","Epoch 2/100\n","----------\n","Phase: train Loss: -143.8801 Acc: 0.6238\n","Phase: val Loss: -198.4808 Acc: 0.6435\n","Epoch 3/100\n","----------\n","Phase: train Loss: -267.3941 Acc: 0.6539\n","Phase: val Loss: -320.7415 Acc: 0.6481\n","Epoch 4/100\n","----------\n","Phase: train Loss: -389.5348 Acc: 0.6366\n","Phase: val Loss: -444.2230 Acc: 0.6343\n","Epoch 5/100\n","----------\n","Phase: train Loss: -514.5981 Acc: 0.6377\n","Phase: val Loss: -561.9692 Acc: 0.6435\n","Epoch 6/100\n","----------\n","Phase: train Loss: -638.0148 Acc: 0.6227\n","Phase: val Loss: -677.4919 Acc: 0.6481\n","Epoch 7/100\n","----------\n","Phase: train Loss: -764.4066 Acc: 0.6238\n","Phase: val Loss: -805.2958 Acc: 0.6528\n","Epoch 8/100\n","----------\n","Phase: train Loss: -884.5601 Acc: 0.6250\n","Phase: val Loss: -925.2132 Acc: 0.6435\n","Epoch 9/100\n","----------\n","Phase: train Loss: -1013.3043 Acc: 0.6458\n","Phase: val Loss: -1043.9694 Acc: 0.6435\n","Epoch 10/100\n","----------\n","Phase: train Loss: -1141.5556 Acc: 0.6262\n","Phase: val Loss: -1157.2370 Acc: 0.6528\n","Epoch 11/100\n","----------\n","Phase: train Loss: -1268.0487 Acc: 0.6470\n","Phase: val Loss: -1260.8936 Acc: 0.6574\n","Epoch 12/100\n","----------\n","Phase: train Loss: -1383.4824 Acc: 0.6377\n","Phase: val Loss: -1403.9244 Acc: 0.6481\n","Epoch 13/100\n","----------\n","Phase: train Loss: -1514.7650 Acc: 0.6493\n","Phase: val Loss: -1520.0175 Acc: 0.6435\n","Epoch 14/100\n","----------\n","Phase: train Loss: -1654.8965 Acc: 0.6389\n","Phase: val Loss: -1634.3401 Acc: 0.6389\n","Epoch 15/100\n","----------\n","Phase: train Loss: -1762.0302 Acc: 0.6505\n","Phase: val Loss: -1753.3386 Acc: 0.6528\n","Epoch 16/100\n","----------\n","Phase: train Loss: -1890.3337 Acc: 0.6412\n","Phase: val Loss: -1878.0949 Acc: 0.6435\n","Epoch 17/100\n","----------\n","Phase: train Loss: -2020.0974 Acc: 0.6470\n","Phase: val Loss: -1993.6601 Acc: 0.6620\n","Epoch 18/100\n","----------\n","Phase: train Loss: -2130.0811 Acc: 0.6273\n","Phase: val Loss: -2141.2445 Acc: 0.6528\n","Epoch 19/100\n","----------\n","Phase: train Loss: -2260.3442 Acc: 0.6412\n","Phase: val Loss: -2232.7451 Acc: 0.6528\n","Epoch 20/100\n","----------\n","Phase: train Loss: -2390.3150 Acc: 0.6470\n","Phase: val Loss: -2344.1878 Acc: 0.6481\n","Epoch 21/100\n","----------\n","Phase: train Loss: -2515.8200 Acc: 0.6377\n","Phase: val Loss: -2443.5686 Acc: 0.6435\n","Epoch 22/100\n","----------\n","Phase: train Loss: -2639.1454 Acc: 0.6435\n","Phase: val Loss: -2580.6100 Acc: 0.6481\n","Epoch 23/100\n","----------\n","Phase: train Loss: -2755.7468 Acc: 0.6273\n","Phase: val Loss: -2731.5583 Acc: 0.6435\n","Epoch 24/100\n","----------\n","Phase: train Loss: -2869.2609 Acc: 0.6528\n","Phase: val Loss: -2843.8943 Acc: 0.6620\n","Epoch 25/100\n","----------\n","Phase: train Loss: -3018.8779 Acc: 0.6424\n","Phase: val Loss: -2992.5119 Acc: 0.6528\n","Epoch 26/100\n","----------\n","Phase: train Loss: -3139.0094 Acc: 0.6551\n","Phase: val Loss: -3067.4045 Acc: 0.6389\n","Epoch 27/100\n","----------\n","Phase: train Loss: -3241.8360 Acc: 0.6412\n","Phase: val Loss: -3171.2992 Acc: 0.6528\n","Epoch 28/100\n","----------\n","Phase: train Loss: -3379.9966 Acc: 0.6597\n","Phase: val Loss: -3307.9763 Acc: 0.6528\n","Epoch 29/100\n","----------\n","Phase: train Loss: -3493.2506 Acc: 0.6412\n","Phase: val Loss: -3428.8263 Acc: 0.6296\n","Epoch 30/100\n","----------\n","Phase: train Loss: -3595.3508 Acc: 0.6308\n","Phase: val Loss: -3559.5376 Acc: 0.6481\n","Epoch 31/100\n","----------\n","Phase: train Loss: -3770.3293 Acc: 0.6620\n","Phase: val Loss: -3690.6746 Acc: 0.6620\n","Epoch 32/100\n","----------\n","Phase: train Loss: -3831.9384 Acc: 0.6308\n","Phase: val Loss: -3793.5707 Acc: 0.6528\n","Epoch 33/100\n","----------\n","Phase: train Loss: -4038.5405 Acc: 0.6447\n","Phase: val Loss: -3933.2540 Acc: 0.6481\n","Epoch 34/100\n","----------\n","Phase: train Loss: -4122.8204 Acc: 0.6400\n","Phase: val Loss: -4036.7843 Acc: 0.6574\n","Epoch 35/100\n","----------\n","Phase: train Loss: -4252.4539 Acc: 0.6354\n","Phase: val Loss: -4143.5256 Acc: 0.6296\n","Epoch 36/100\n","----------\n","Phase: train Loss: -4415.6556 Acc: 0.6285\n","Phase: val Loss: -4245.7842 Acc: 0.6574\n","Epoch 37/100\n","----------\n","Phase: train Loss: -4480.2114 Acc: 0.6354\n","Phase: val Loss: -4398.7274 Acc: 0.6528\n","Epoch 38/100\n","----------\n","Phase: train Loss: -4584.4207 Acc: 0.6400\n","Phase: val Loss: -4518.9292 Acc: 0.6574\n","Epoch 39/100\n","----------\n","Phase: train Loss: -4781.4186 Acc: 0.6412\n","Phase: val Loss: -4636.1199 Acc: 0.6481\n","Epoch 40/100\n","----------\n","Phase: train Loss: -4902.0456 Acc: 0.6331\n","Phase: val Loss: -4701.4773 Acc: 0.6435\n","Epoch 41/100\n","----------\n","Phase: train Loss: -4992.2569 Acc: 0.6528\n","Phase: val Loss: -4851.3332 Acc: 0.6528\n","Epoch 42/100\n","----------\n","Phase: train Loss: -5076.3238 Acc: 0.6412\n","Phase: val Loss: -4986.2147 Acc: 0.6481\n","Epoch 43/100\n","----------\n","Phase: train Loss: -5253.6639 Acc: 0.6458\n","Phase: val Loss: -5140.9178 Acc: 0.6343\n","Epoch 44/100\n","----------\n","Phase: train Loss: -5351.5123 Acc: 0.6366\n","Phase: val Loss: -5178.7745 Acc: 0.6343\n","Epoch 45/100\n","----------\n","Phase: train Loss: -5477.7425 Acc: 0.6516\n","Phase: val Loss: -5322.6032 Acc: 0.6435\n","Epoch 46/100\n","----------\n","Phase: train Loss: -5577.6726 Acc: 0.6528\n","Phase: val Loss: -5496.1509 Acc: 0.6204\n","Epoch 47/100\n","----------\n","Phase: train Loss: -5726.1332 Acc: 0.6296\n","Phase: val Loss: -5637.6644 Acc: 0.6574\n","Epoch 48/100\n","----------\n","Phase: train Loss: -5864.4753 Acc: 0.6458\n","Phase: val Loss: -5736.5604 Acc: 0.6528\n","Epoch 49/100\n","----------\n","Phase: train Loss: -5921.5571 Acc: 0.6215\n","Phase: val Loss: -5886.1505 Acc: 0.6528\n","Epoch 50/100\n","----------\n","Phase: train Loss: -6105.2427 Acc: 0.6551\n","Phase: val Loss: -5987.6214 Acc: 0.6435\n","Epoch 51/100\n","----------\n","Phase: train Loss: -6258.5235 Acc: 0.6424\n","Phase: val Loss: -6094.5737 Acc: 0.6574\n","Epoch 52/100\n","----------\n","Phase: train Loss: -6382.7827 Acc: 0.6493\n","Phase: val Loss: -6193.6914 Acc: 0.6481\n","Epoch 53/100\n","----------\n","Phase: train Loss: -6536.8011 Acc: 0.6435\n","Phase: val Loss: -6353.6850 Acc: 0.6435\n","Epoch 54/100\n","----------\n","Phase: train Loss: -6658.9474 Acc: 0.6435\n","Phase: val Loss: -6411.7660 Acc: 0.6574\n","Epoch 55/100\n","----------\n","Phase: train Loss: -6701.3703 Acc: 0.6458\n","Phase: val Loss: -6598.5572 Acc: 0.6435\n","Epoch 56/100\n","----------\n","Phase: train Loss: -6839.4815 Acc: 0.6354\n","Phase: val Loss: -6771.3169 Acc: 0.6389\n","Epoch 57/100\n","----------\n","Phase: train Loss: -6938.1686 Acc: 0.6273\n","Phase: val Loss: -6761.5307 Acc: 0.6528\n","Epoch 58/100\n","----------\n","Phase: train Loss: -7136.9160 Acc: 0.6400\n","Phase: val Loss: -6988.5080 Acc: 0.6620\n","Epoch 59/100\n","----------\n","Phase: train Loss: -7248.8980 Acc: 0.6516\n","Phase: val Loss: -7044.8468 Acc: 0.6574\n","Epoch 60/100\n","----------\n","Phase: train Loss: -7383.5070 Acc: 0.6493\n","Phase: val Loss: -7260.5789 Acc: 0.6296\n","Epoch 61/100\n","----------\n","Phase: train Loss: -7497.1725 Acc: 0.6354\n","Phase: val Loss: -7249.2254 Acc: 0.6528\n","Epoch 62/100\n","----------\n","Phase: train Loss: -7573.6679 Acc: 0.6400\n","Phase: val Loss: -7476.2831 Acc: 0.6574\n","Epoch 63/100\n","----------\n","Phase: train Loss: -7799.8196 Acc: 0.6493\n","Phase: val Loss: -7500.6141 Acc: 0.6574\n","Epoch 64/100\n","----------\n","Phase: train Loss: -7905.9611 Acc: 0.6238\n","Phase: val Loss: -7632.1981 Acc: 0.6389\n","Epoch 65/100\n","----------\n","Phase: train Loss: -7951.6596 Acc: 0.6447\n","Phase: val Loss: -7804.9922 Acc: 0.6528\n","Epoch 66/100\n","----------\n","Phase: train Loss: -8206.6143 Acc: 0.6481\n","Phase: val Loss: -7845.8847 Acc: 0.6528\n","Epoch 67/100\n","----------\n","Phase: train Loss: -8177.9810 Acc: 0.6586\n","Phase: val Loss: -8005.9134 Acc: 0.6481\n","Epoch 68/100\n","----------\n","Phase: train Loss: -8350.3136 Acc: 0.6481\n","Phase: val Loss: -8071.1383 Acc: 0.6296\n","Epoch 69/100\n","----------\n","Phase: train Loss: -8406.3918 Acc: 0.6400\n","Phase: val Loss: -8197.2152 Acc: 0.6435\n","Epoch 70/100\n","----------\n","Phase: train Loss: -8565.2431 Acc: 0.6493\n","Phase: val Loss: -8373.6817 Acc: 0.6250\n","Epoch 71/100\n","----------\n","Phase: train Loss: -8670.1933 Acc: 0.6343\n","Phase: val Loss: -8543.6462 Acc: 0.6343\n","Epoch 72/100\n","----------\n","Phase: train Loss: -8829.3317 Acc: 0.6470\n","Phase: val Loss: -8619.3096 Acc: 0.6667\n","Epoch 73/100\n","----------\n","Phase: train Loss: -8982.4359 Acc: 0.6470\n","Phase: val Loss: -8765.8487 Acc: 0.6574\n","Epoch 74/100\n","----------\n","Phase: train Loss: -9150.4580 Acc: 0.6331\n","Phase: val Loss: -8867.3927 Acc: 0.6389\n","Epoch 75/100\n","----------\n","Phase: train Loss: -9221.7846 Acc: 0.6366\n","Phase: val Loss: -8978.4468 Acc: 0.6528\n","Epoch 76/100\n","----------\n","Phase: train Loss: -9300.7993 Acc: 0.6377\n","Phase: val Loss: -9083.3936 Acc: 0.6389\n","Epoch 77/100\n","----------\n","Phase: train Loss: -9371.3837 Acc: 0.6400\n","Phase: val Loss: -9235.2298 Acc: 0.6713\n","Epoch 78/100\n","----------\n","Phase: train Loss: -9568.2241 Acc: 0.6447\n","Phase: val Loss: -9345.6928 Acc: 0.6435\n","Epoch 79/100\n","----------\n","Phase: train Loss: -9731.0647 Acc: 0.6389\n","Phase: val Loss: -9435.8551 Acc: 0.6481\n","Epoch 80/100\n","----------\n","Phase: train Loss: -9747.6387 Acc: 0.6505\n","Phase: val Loss: -9482.8886 Acc: 0.6435\n","Epoch 81/100\n","----------\n","Phase: train Loss: -9971.6958 Acc: 0.6435\n","Phase: val Loss: -9685.7909 Acc: 0.6389\n","Epoch 82/100\n","----------\n","Phase: train Loss: -10146.8464 Acc: 0.6424\n","Phase: val Loss: -9860.3779 Acc: 0.6667\n","Epoch 83/100\n","----------\n","Phase: train Loss: -10156.6728 Acc: 0.6389\n","Phase: val Loss: -9940.0360 Acc: 0.6481\n","Epoch 84/100\n","----------\n","Phase: train Loss: -10341.1870 Acc: 0.6412\n","Phase: val Loss: -9939.6895 Acc: 0.6574\n","Epoch 85/100\n","----------\n","Phase: train Loss: -10327.5947 Acc: 0.6343\n","Phase: val Loss: -10237.6687 Acc: 0.6574\n","Epoch 86/100\n","----------\n","Phase: train Loss: -10628.0583 Acc: 0.6447\n","Phase: val Loss: -10334.6493 Acc: 0.6435\n","Epoch 87/100\n","----------\n","Phase: train Loss: -10686.6916 Acc: 0.6435\n","Phase: val Loss: -10286.9738 Acc: 0.6481\n","Epoch 88/100\n","----------\n","Phase: train Loss: -10769.6633 Acc: 0.6412\n","Phase: val Loss: -10457.3970 Acc: 0.6296\n","Epoch 89/100\n","----------\n","Phase: train Loss: -10985.0976 Acc: 0.6400\n","Phase: val Loss: -10533.1885 Acc: 0.6389\n","Epoch 90/100\n","----------\n","Phase: train Loss: -10989.6883 Acc: 0.6354\n","Phase: val Loss: -10753.4097 Acc: 0.6296\n","Epoch 91/100\n","----------\n","Phase: train Loss: -11221.2644 Acc: 0.6308\n","Phase: val Loss: -10969.6626 Acc: 0.6343\n","Epoch 92/100\n","----------\n","Phase: train Loss: -11280.1636 Acc: 0.6470\n","Phase: val Loss: -11006.0488 Acc: 0.6389\n","Epoch 93/100\n","----------\n","Phase: train Loss: -11538.5659 Acc: 0.6412\n","Phase: val Loss: -11129.7878 Acc: 0.6435\n","Epoch 94/100\n","----------\n","Phase: train Loss: -11512.2831 Acc: 0.6400\n","Phase: val Loss: -11340.1493 Acc: 0.6296\n","Epoch 95/100\n","----------\n","Phase: train Loss: -11692.5966 Acc: 0.6481\n","Phase: val Loss: -11313.8609 Acc: 0.6296\n","Epoch 96/100\n","----------\n","Phase: train Loss: -11894.9115 Acc: 0.6539\n","Phase: val Loss: -11437.8378 Acc: 0.6528\n","Epoch 97/100\n","----------\n","Phase: train Loss: -11953.5154 Acc: 0.6250\n","Phase: val Loss: -11722.1733 Acc: 0.6389\n","Epoch 98/100\n","----------\n","Phase: train Loss: -12058.3571 Acc: 0.6481\n","Phase: val Loss: -11655.1253 Acc: 0.6481\n","Epoch 99/100\n","----------\n","Phase: train Loss: -12229.6424 Acc: 0.6516\n","Phase: val Loss: -11980.9617 Acc: 0.6667\n","Epoch 100/100\n","----------\n","Phase: train Loss: -12360.2030 Acc: 0.6331\n","Phase: val Loss: -11940.5440 Acc: 0.6481\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["VGG(\n","  (features): Sequential(\n","    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): ReLU(inplace=True)\n","    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (3): ReLU(inplace=True)\n","    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (6): ReLU(inplace=True)\n","    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (8): ReLU(inplace=True)\n","    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (11): ReLU(inplace=True)\n","    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (13): ReLU(inplace=True)\n","    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (15): ReLU(inplace=True)\n","    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (18): ReLU(inplace=True)\n","    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (20): ReLU(inplace=True)\n","    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (22): ReLU(inplace=True)\n","    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (25): ReLU(inplace=True)\n","    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (27): ReLU(inplace=True)\n","    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (29): ReLU(inplace=True)\n","    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  )\n","  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n","  (classifier): Sequential(\n","    (0): Linear(in_features=25088, out_features=4096, bias=True)\n","    (1): ReLU(inplace=True)\n","    (2): Dropout(p=0.5, inplace=False)\n","    (3): Linear(in_features=4096, out_features=4096, bias=True)\n","    (4): ReLU(inplace=True)\n","    (5): Dropout(p=0.5, inplace=False)\n","    (6): Linear(in_features=4096, out_features=6, bias=True)\n","  )\n","  (fc): Linear(in_features=4096, out_features=6, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":115}]},{"cell_type":"code","metadata":{"id":"KW5rJstP5sIT","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}